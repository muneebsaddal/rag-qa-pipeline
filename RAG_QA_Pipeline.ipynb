{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "998fbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "json_dev = pd.read_json(\"data/dev-v2.0.json\")\n",
    "json_train = pd.read_json(\"data/train-v2.0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b2ffba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extraction(json_data):\n",
    "    extracted_data = []\n",
    "\n",
    "    # Iterate through each item in the 'data' column\n",
    "    for item in json_data['data']:\n",
    "        title = item['title']  # Access 'title' within the item\n",
    "        for paragraph in item['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                # Assuming we take the first answer provided for each question\n",
    "                if qa['answers']:\n",
    "                    answer_text = qa['answers'][0]['text']\n",
    "                    answer_start = qa['answers'][0]['answer_start']\n",
    "                    answer_end = answer_start + len(answer_text)\n",
    "                else:\n",
    "                    # Handle cases where there might be no answers (e.g., for is_impossible=True)\n",
    "                    answer_text = None\n",
    "                    answer_start = None\n",
    "                    answer_end = None\n",
    "\n",
    "                extracted_data.append({\n",
    "                    'title': title,\n",
    "                    'context': context,\n",
    "                    'question': question,\n",
    "                    'answer': answer_text,\n",
    "                    'answer_start': answer_start,\n",
    "                    'answer_end': answer_end\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1910059e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_columns = data_extraction(json_dev)\n",
    "train_columns = data_extraction(json_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f37c1267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\munee\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\munee\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Total chunks created for retrieval: 5612\n",
      "\n",
      "Sample of the new chunked DataFrame:\n",
      "        title                                      context_chunk chunk_id\n",
      "0     Normans  The Normans (Norman: Nourmands; French: Norman...      0_0\n",
      "1     Normans  . They were descended from Norse (\"Norman\" com...      0_1\n",
      "2     Normans  . Through generations of assimilation and mixi...      0_2\n",
      "3     Normans  . The distinct cultural and ethnic identity of...      0_3\n",
      "4     Normans  The Norman dynasty had a major political, cult...      1_0\n",
      "...       ...                                                ...      ...\n",
      "5607    Force  . According to the Second law of thermodynamic...   1202_2\n",
      "5608    Force  The pound-force has a metric counterpart, less...   1203_0\n",
      "5609    Force  . The kilogram-force leads to an alternate, bu...   1203_1\n",
      "5610    Force  . The kilogram-force is not a part of the mode...   1203_2\n",
      "5611    Force  . Other arcane units of force include the sthè...   1203_3\n",
      "\n",
      "[5612 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def create_recursive_chunks(df, chunk_size=256, chunk_overlap=32):\n",
    "    \"\"\"\n",
    "    Splits the unique contexts of a DataFrame into semantic chunks using\n",
    "    the recommended RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing 'title' and 'context' columns.\n",
    "        chunk_size (int): The target size for each chunk in characters.\n",
    "        chunk_overlap (int): The number of characters to overlap between chunks.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with columns ['title', 'context_chunk', 'chunk_id'].\n",
    "    \"\"\"\n",
    "    # 1. Initialize the RecursiveCharacterTextSplitter\n",
    "    # This splitter attempts to split text based on a prioritized list of separators.\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        # This list of separators is key. It tries to split by paragraph,\n",
    "        # then by sentence, then by space, ensuring chunks are as\n",
    "        # semantically coherent as possible.\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "        length_function=len\n",
    "    )\n",
    "\n",
    "    chunked_data = []\n",
    "    \n",
    "    # 2. Process only unique contexts to avoid redundant work\n",
    "    unique_contexts_df = df.drop_duplicates(subset=['title', 'context']).reset_index(drop=True)\n",
    "\n",
    "    # 3. Iterate over each unique document\n",
    "    for index, row in unique_contexts_df.iterrows():\n",
    "        title = row['title']\n",
    "        context = row['context']\n",
    "        \n",
    "        # 4. Use the splitter to create chunks from the context\n",
    "        chunks = splitter.split_text(context)\n",
    "\n",
    "        # 5. Create a new record for each chunk\n",
    "        for i, chunk_text in enumerate(chunks):\n",
    "            chunked_data.append({\n",
    "                'title': title,\n",
    "                'context_chunk': chunk_text,\n",
    "                'chunk_id': f\"{index}_{i}\" # A unique ID for tracking each chunk\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(chunked_data)\n",
    "\n",
    "# --- Parameters and Execution ---\n",
    "CHUNK_SIZE = 256\n",
    "CHUNK_OVERLAP = 32\n",
    "\n",
    "# Call the function to create the new chunked DataFrame\n",
    "df_chunked_for_retrieval = create_recursive_chunks(dev_columns, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "# --- Display the results ---\n",
    "print(f\"Total chunks created for retrieval: {len(df_chunked_for_retrieval)}\\n\")\n",
    "print(\"Sample of the new chunked DataFrame:\")\n",
    "print(df_chunked_for_retrieval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "399ba7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading sentence transformer model: all-MiniLM-L6-v2...\n",
      "Model loaded successfully.\n",
      "Encoding 5612 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 176/176 [00:37<00:00,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding complete.\n",
      "Shape of embeddings array: (5612, 384)\n",
      "Building FAISS index...\n",
      "FAISS index built. Total vectors in index: 5612\n",
      "Saving artifacts to disk...\n",
      "- Saved embeddings to models/embeddings.pkl\n",
      "- Saved FAISS index to models/faiss.index\n",
      "- Saved metadata to data/metadata.pkl\n",
      "\n",
      "All steps completed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- 1. Choose and Load Embedding Model ---\n",
    "print(\"Loading sentence transformer model: all-MiniLM-L6-v2...\")\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "# We recommend using a GPU for encoding, so specify 'cuda' if available\n",
    "# If not, 'cpu' will be used automatically.\n",
    "model = SentenceTransformer(model_name, device='cuda' if 'cuda' in os.environ.get('CUDA_VISIBLE_DEVICES', '') else 'cpu')\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "\n",
    "# --- 2. Encode All Chunks ---\n",
    "# Extract the text chunks to be encoded\n",
    "chunks_to_encode = df_chunked_for_retrieval['context_chunk'].tolist()\n",
    "\n",
    "print(f\"Encoding {len(chunks_to_encode)} chunks...\")\n",
    "# The model.encode() method converts text into a NumPy array of embeddings\n",
    "# show_progress_bar=True provides a visual indicator of progress.\n",
    "embeddings = model.encode(chunks_to_encode, show_progress_bar=True)\n",
    "print(\"Encoding complete.\")\n",
    "print(f\"Shape of embeddings array: {embeddings.shape}\")\n",
    "\n",
    "\n",
    "# --- 3. Store Embeddings in NumPy Array (already done by model.encode) ---\n",
    "# The 'embeddings' variable is already the NumPy array you need.\n",
    "\n",
    "\n",
    "# --- 4. Build FAISS Index ---\n",
    "print(\"Building FAISS index...\")\n",
    "# Get the dimensionality of the embeddings\n",
    "embedding_dim = embeddings.shape[1]\n",
    "\n",
    "# For 'all-MiniLM-L6-v2', embeddings are normalized.\n",
    "# This means Inner Product (IP) is equivalent to Cosine Similarity.\n",
    "# IndexFlatIP is a fast index for this purpose.\n",
    "index = faiss.IndexFlatIP(embedding_dim)\n",
    "\n",
    "# Add the embeddings to the index\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index built. Total vectors in index: {index.ntotal}\")\n",
    "\n",
    "\n",
    "# --- 5. Save the Artifacts ---\n",
    "print(\"Saving artifacts to disk...\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# Save embeddings array\n",
    "with open('models/embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "print(\"- Saved embeddings to models/embeddings.pkl\")\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(index, 'models/faiss.index')\n",
    "print(\"- Saved FAISS index to models/faiss.index\")\n",
    "\n",
    "# Save metadata (the chunked DataFrame)\n",
    "# This is crucial for mapping search results back to the original text.\n",
    "df_chunked_for_retrieval.to_pickle('data/metadata.pkl')\n",
    "print(\"- Saved metadata to data/metadata.pkl\")\n",
    "\n",
    "print(\"\\nAll steps completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f7093c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "        title                                      context_chunk chunk_id\n",
      "0     Normans  The Normans (Norman: Nourmands; French: Norman...      0_0\n",
      "1     Normans  . They were descended from Norse (\"Norman\" com...      0_1\n",
      "2     Normans  . Through generations of assimilation and mixi...      0_2\n",
      "3     Normans  . The distinct cultural and ethnic identity of...      0_3\n",
      "4     Normans  The Norman dynasty had a major political, cult...      1_0\n",
      "...       ...                                                ...      ...\n",
      "5607    Force  . According to the Second law of thermodynamic...   1202_2\n",
      "5608    Force  The pound-force has a metric counterpart, less...   1203_0\n",
      "5609    Force  . The kilogram-force leads to an alternate, bu...   1203_1\n",
      "5610    Force  . The kilogram-force is not a part of the mode...   1203_2\n",
      "5611    Force  . Other arcane units of force include the sthè...   1203_3\n",
      "\n",
      "[5612 rows x 3 columns]\n",
      "Loading embedding model: all-MiniLM-L6-v2...\n",
      "Model loaded successfully.\n",
      "Encoding 5612 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 176/176 [00:49<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks encoded.\n",
      "Embeddings shape: (5612, 384)\n",
      "Normalizing embeddings for cosine similarity...\n",
      "Embeddings normalized.\n",
      "Building FAISS IndexFlatL2 with dimension 384...\n",
      "FAISS index built with 5612 vectors.\n",
      "Saving embeddings to models/embeddings.pkl...\n",
      "Embeddings saved.\n",
      "Saving FAISS index to models/faiss.index...\n",
      "FAISS index saved.\n",
      "Saving metadata to data/metadata.pkl...\n",
      "Metadata saved.\n",
      "\n",
      "Process completed successfully!\n",
      "\n",
      "--- Verifying saved files ---\n",
      "Loaded embeddings shape: (5612, 384)\n",
      "Loaded FAISS index total vectors: 5612\n",
      "Loaded metadata count: 5612\n",
      "\n",
      "Top 3 results for query: 'What is AI?'\n",
      "Error during verification: np.int64(183)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "def create_and_save_faiss_index(\n",
    "    chunks: List[str],\n",
    "    metadata: List[Dict[str, Any]],\n",
    "    model_name: str = 'all-MiniLM-L6-v2',\n",
    "    embeddings_path: str = 'models/embeddings.pkl',\n",
    "    faiss_index_path: str = 'models/faiss.index',\n",
    "    metadata_path: str = 'data/metadata.pkl'\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Encodes text chunks, builds a FAISS index for cosine similarity,\n",
    "    and saves the embeddings, FAISS index, and metadata.\n",
    "\n",
    "    Args:\n",
    "        chunks (List[str]): A list of text strings to be embedded.\n",
    "        metadata (List[Dict[str, Any]]): A list of dictionaries, where each dictionary\n",
    "                                          corresponds to the metadata for a chunk.\n",
    "        model_name (str): The name of the SentenceTransformer model to use.\n",
    "        embeddings_path (str): The file path to save the embeddings NumPy array.\n",
    "        faiss_index_path (str): The file path to save the FAISS index.\n",
    "        metadata_path (str): The file path to save the metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Choose embedding model\n",
    "    print(f\"Loading embedding model: {model_name}...\")\n",
    "    model = SentenceTransformer(model_name)\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(os.path.dirname(embeddings_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(metadata_path), exist_ok=True)\n",
    "\n",
    "    # 2. Encode all chunks\n",
    "    print(f\"Encoding {len(chunks)} chunks...\")\n",
    "    embeddings = model.encode(chunks, show_progress_bar=True)\n",
    "    print(\"Chunks encoded.\")\n",
    "\n",
    "    # 3. Store embeddings in NumPy array\n",
    "    # Ensure embeddings are float32, which is standard for FAISS\n",
    "    embeddings_np = np.array(embeddings).astype('float32')\n",
    "    print(f\"Embeddings shape: {embeddings_np.shape}\")\n",
    "\n",
    "    # Get embedding dimension\n",
    "    d = embeddings_np.shape[1]\n",
    "\n",
    "    print(\"Normalizing embeddings for cosine similarity...\")\n",
    "    faiss.normalize_L2(embeddings_np)\n",
    "    print(\"Embeddings normalized.\")\n",
    "\n",
    "    print(f\"Building FAISS IndexFlatL2 with dimension {d}...\")\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "\n",
    "    num_vectors = embeddings_np.shape[0]\n",
    "    index.add(x=embeddings_np)\n",
    "\n",
    "    print(f\"FAISS index built with {index.ntotal} vectors.\")\n",
    "    # 5. Save:\n",
    "    # 5.1. Save embeddings.pkl\n",
    "    print(f\"Saving embeddings to {embeddings_path}...\")\n",
    "    with open(embeddings_path, 'wb') as f:\n",
    "        pickle.dump(embeddings_np, f)\n",
    "    print(\"Embeddings saved.\")\n",
    "\n",
    "    # 5.2. Save faiss.index\n",
    "    print(f\"Saving FAISS index to {faiss_index_path}...\")\n",
    "    faiss.write_index(index, faiss_index_path)\n",
    "    print(\"FAISS index saved.\")\n",
    "\n",
    "    # 5.3. Save metadata.pkl\n",
    "    print(f\"Saving metadata to {metadata_path}...\")\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(\"Metadata saved.\")\n",
    "\n",
    "    print(\"\\nProcess completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pickle\n",
    "\n",
    "    # Define the file path\n",
    "    file_path = 'data/metadata.pkl'\n",
    "\n",
    "    # Open the file in binary read mode and load the data\n",
    "    with open(file_path, 'rb') as file:\n",
    "        metadata = pickle.load(file)\n",
    "\n",
    "    # Now 'loaded_list' is a Python list containing the data from the .pkl file\n",
    "    print(type(metadata))\n",
    "    print(metadata)\n",
    "\n",
    "    create_and_save_faiss_index(\n",
    "        chunks=df_chunked_for_retrieval['context_chunk'].tolist(),\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "    # --- Verification (Optional) ---\n",
    "    print(\"\\n--- Verifying saved files ---\")\n",
    "    try:\n",
    "        # Load embeddings\n",
    "        with open('models/embeddings.pkl', 'rb') as f:\n",
    "            loaded_embeddings = pickle.load(f)\n",
    "        print(f\"Loaded embeddings shape: {loaded_embeddings.shape}\")\n",
    "\n",
    "        # Load FAISS index\n",
    "        loaded_index = faiss.read_index('models/faiss.index')\n",
    "        print(f\"Loaded FAISS index total vectors: {loaded_index.ntotal}\")\n",
    "\n",
    "        # Load metadata\n",
    "        with open('data/metadata.pkl', 'rb') as f:\n",
    "            loaded_metadata = pickle.load(f)\n",
    "        print(f\"Loaded metadata count: {len(loaded_metadata)}\")\n",
    "\n",
    "        # Example search (optional)\n",
    "        query_text = \"What is AI?\"\n",
    "        query_embedding = SentenceTransformer('all-MiniLM-L6-v2').encode([query_text]).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding) # Normalize query embedding as well\n",
    "\n",
    "        k = 3 # Number of nearest neighbors to retrieve\n",
    "        distances, indices = loaded_index.search(query_embedding, k)\n",
    "\n",
    "        print(f\"\\nTop {k} results for query: '{query_text}'\")\n",
    "        for i in range(k):\n",
    "            print(f\"  Rank {i+1}: Chunk ID {loaded_metadata[indices[0][i]]['id']} (Distance: {distances[0][i]:.4f})\")\n",
    "            print(f\"    Text: {df_chunked_for_retrieval['context_chunk'].tolist()[indices[0][i]]}\")\n",
    "            print(f\"    Metadata: {loaded_metadata[indices[0][i]]}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during verification: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86528211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunks_file = df_chunked_for_retrieval['context_chunk']\n",
    "chunks_file.to_csv('output_pandas.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
